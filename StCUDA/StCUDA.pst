<?xml version="1.0"?><st-source><!-- Name: StCUDAComment: StCUDA allows Smalltalk to call CUDA Driver APIs to drive GPU computing.This software is open source software available under the MIT license.Copyright (c) 2018, Los Alamos National Security, LLC.All rights reserved.Copyright (c) 2018, Los Alamos National Security, LLC. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.DevelopmentPrerequisites: #(#(#any 'ASKOH' '') #(#any 'Base VisualWorks' '') #(#any 'DLLCC' ''))PackageName: StCUDAParcel: #('StCUDA')ParcelDirectory: StCUDAParcelName: StCUDAPrerequisiteDescriptions: #(#(#name 'ASKOH' #componentType #package) #(#name 'Base VisualWorks' #componentType #bundle) #(#name 'DLLCC' #componentType #package))PrerequisiteParcels: #(#('ASKOH' '') #('Base VisualWorks' '') #('DLLCC' ''))Version: 8.3.1.1Date: 4:56:42 PM December 5, 2018 --><time-stamp>From VisualWorksÂ®, 8.3.1 of March 23, 2018 on December 5, 2018 at 4:56:42 PM</time-stamp><do-it>(Dialog confirm: 'You are filing-in a Parcel source file!\\While this is possible it will not have\the same effect as loading the parcel.\None of the Parcel''s prerequisites will\be loaded and none of its load actions\will be performed.\\Are you sure you want to file-in?' withCRs) ifFalse: [self error: 'Parcel file-in abandoned.  Choose terminate or close.']</do-it><class><name>StCUDA</name><environment>Smalltalk</environment><super>Core.Object</super><private>false</private><indexed-type>none</indexed-type><inst-vars>deviceCountPtr devID majorPtr minorPtr maxMajor maxFlops bestDevID cuDevicePtr deviceNamePtr deviceTotalMemPtr cuContextPtr compileOptions jitOptions ptxOrcuFile ptxSourcePtr functionName functionNamePtr cuModulePtr cuFunctionPtr gridDimX gridDimY gridDimZ blockDimX blockDimY blockDimZ sharedMemBytes kernelParams </inst-vars><class-inst-vars></class-inst-vars><imports></imports><category>StCUDA</category><attributes><package>StCUDA</package></attributes></class><comment><class-id>StCUDA</class-id><body>StCUDA allows Smalltalk to call CUDA Driver APIs to drive GPU computing.This software is open source software available under the MIT license.Copyright (c) 2018, Los Alamos National Security, LLC.All rights reserved.Copyright (c) 2018, Los Alamos National Security, LLC. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</body></comment><class><name>StCUDAVectorAdd</name><environment>Smalltalk</environment><super>StCUDA</super><private>false</private><indexed-type>none</indexed-type><inst-vars></inst-vars><class-inst-vars></class-inst-vars><imports></imports><category>StCUDA</category><attributes><package>StCUDA</package></attributes></class><comment><class-id>StCUDAVectorAdd</class-id><body>StCUDA allows Smalltalk to call CUDA Driver APIs to drive GPU computing.This software is open source software available under the MIT license.Copyright (c) 2018, Los Alamos National Security, LLC.All rights reserved.Copyright (c) 2018, Los Alamos National Security, LLC. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</body></comment><class><name>StCUDAMatrixMul</name><environment>Smalltalk</environment><super>StCUDA</super><private>false</private><indexed-type>none</indexed-type><inst-vars></inst-vars><class-inst-vars></class-inst-vars><imports></imports><category>StCUDA</category><attributes><package>StCUDA</package></attributes></class><comment><class-id>StCUDAMatrixMul</class-id><body>StCUDA allows Smalltalk to call CUDA Driver APIs to drive GPU computing.This software is open source software available under the MIT license.Copyright (c) 2018, Los Alamos National Security, LLC.All rights reserved.Copyright (c) 2018, Los Alamos National Security, LLC. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</body></comment><class><name>StCUDAWin64</name><environment>Smalltalk</environment><super>External.ExternalInterface</super><private>false</private><indexed-type>none</indexed-type><inst-vars></inst-vars><class-inst-vars></class-inst-vars><imports>			private StCUDAWin64Dictionary.*			</imports><category>StCUDA</category><attributes><includeFiles><item>cuda.h</item> <item>nvrtc.h</item> <item>cuda_runtime.h</item> <item>builtin_types.h</item></includeFiles><includeDirectories><item>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\include</item></includeDirectories><libraryFiles><item>nvcuda.dll</item> <item>nvrtc64_92.dll</item> <item>cudart64_92.dll</item></libraryFiles><libraryDirectories><item>C:\Windows\System32</item> <item>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin</item></libraryDirectories><beVirtual>false</beVirtual><optimizationLevel>full</optimizationLevel><package>StCUDA</package></attributes></class><comment><class-id>StCUDAWin64</class-id><body>StCUDA allows Smalltalk to call CUDA Driver APIs to drive GPU computing.This software is open source software available under the MIT license.Copyright (c) 2018, Los Alamos National Security, LLC.All rights reserved.Copyright (c) 2018, Los Alamos National Security, LLC. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</body></comment><class><name>StCUDALinux64</name><environment>Smalltalk</environment><super>External.ExternalInterface</super><private>false</private><indexed-type>none</indexed-type><inst-vars></inst-vars><class-inst-vars></class-inst-vars><imports>			private StCUDALinux64Dictionary.*			</imports><category>StCUDA</category><attributes><includeFiles><item>cuda.h</item> <item>nvrtc.h</item> <item>cuda_runtime.h</item> <item>builtin_types.h</item></includeFiles><includeDirectories></includeDirectories><libraryFiles><item>libcuda.so</item> <item>libnvrtc.so</item> <item>libcudart.so</item></libraryFiles><libraryDirectories><item>/usr/local/cuda-9.2/targets/x86_64-linux/lib/stubs</item></libraryDirectories><beVirtual>false</beVirtual><optimizationLevel>full</optimizationLevel><package>StCUDA</package></attributes></class><comment><class-id>StCUDALinux64</class-id><body>StCUDA allows Smalltalk to call CUDA Driver APIs to drive GPU computing.This software is open source software available under the MIT license.Copyright (c) 2018, Los Alamos National Security, LLC.All rights reserved.Copyright (c) 2018, Los Alamos National Security, LLC. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</body></comment><shared-variable><name>StCUDAMatrixMulDictionary</name><environment>Smalltalk</environment><private>false</private><constant>false</constant><category>external dictionaries</category><attributes><package>StCUDA</package></attributes></shared-variable><shared-variable><name>StCUDALinux64Dictionary</name><environment>Smalltalk</environment><private>false</private><constant>false</constant><category>external dictionaries</category><attributes><package>StCUDA</package></attributes></shared-variable><shared-variable><name>StCUDADictionary</name><environment>Smalltalk</environment><private>false</private><constant>false</constant><category>external dictionaries</category><attributes><package>StCUDA</package></attributes></shared-variable><shared-variable><name>StCUDAVectorAddDictionary</name><environment>Smalltalk</environment><private>false</private><constant>false</constant><category>external dictionaries</category><attributes><package>StCUDA</package></attributes></shared-variable><shared-variable><name>StCUDAWin64Dictionary</name><environment>Smalltalk</environment><private>false</private><constant>false</constant><category>external dictionaries</category><attributes><package>StCUDA</package></attributes></shared-variable><shared-variable><name>ICUDA</name><environment>StCUDA</environment><private>false</private><constant>false</constant><category>C interface</category><attributes><package>StCUDA</package></attributes></shared-variable><methods><class-id>StCUDAWin64</class-id> <category>procedures</category><body package="StCUDA">cuCtxCreate: pctx with: flags with: dev	&lt;C: CUresult __stdcall cuCtxCreate(CUcontext *pctx, unsigned int flags, CUdevice dev)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuCtxDestroy: ctx	&lt;C: CUresult __stdcall cuCtxDestroy(CUcontext ctx)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuCtxSynchronize	&lt;C: CUresult __stdcall cuCtxSynchronize(void)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuDeviceComputeCapability: major with: minor with: dev	&lt;C: CUresult __stdcall cuDeviceComputeCapability(int *major, int *minor, CUdevice dev)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuDeviceGet: device with: ordinal	&lt;C: CUresult __stdcall cuDeviceGet(CUdevice *device, int ordinal)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuDeviceGetAttribute: attribute with: device_attribute with: device	&lt;C:CUresult __stdcall cuDeviceGetAttribute ( int* attribute, CUdevice_attribute device_attribute, CUdevice device )&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuDeviceGetCount: arg1	&lt;C: CUresult __stdcall cuDeviceGetCount(int *arg1)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuDeviceGetName: name with: len with: dev	&lt;C: CUresult __stdcall cuDeviceGetName(char *name, int len, CUdevice dev)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuDeviceTotalMem: bytes with: dev	&lt;C:CUresult __stdcall cuDeviceTotalMem ( size_t* bytes, CUdevice dev )&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuInit: Flags	&lt;C: CUresult __stdcall cuInit(unsigned int Flags)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuLaunchKernel: f with: agridDimX with: agridDimY with: agridDimZ with: ablockDimX with: ablockDimY with: ablockDimZ with: asharedMemBytes with: hStream with: akernelParams with: extra	&lt;C:CUresult __stdcall cuLaunchKernel(CUfunction f,                                unsigned int agridDimX,                                unsigned int agridDimY,                                unsigned int agridDimZ,                                unsigned int ablockDimX,                                unsigned int ablockDimY,                                unsigned int ablockDimZ,                                unsigned int asharedMemBytes,                                CUstream hStream,                                void **akernelParams,                                void **extra)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuMemAlloc: dptr with: bytesize 	&lt;C: CUresult __stdcall cuMemAlloc(CUdeviceptr *dptr, size_t bytesize)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuMemFree: dptr	&lt;C: CUresult __stdcall cuMemFree(CUdeviceptr dptr)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuMemcpyDtoH: dstHost with: srcDevice with: ByteCount	&lt;C: CUresult __stdcall cuMemcpyDtoH(void *dstHost, CUdeviceptr srcDevice, size_t ByteCount)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuMemcpyHtoD: dstDevice with: srcHost with: ByteCount	&lt;C: CUresult __stdcall cuMemcpyHtoD(CUdeviceptr dstDevice, const void *srcHost, size_t ByteCount)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuModuleGetFunction: hfunc with: hmod with: name	&lt;C: CUresult __stdcall cuModuleGetFunction(CUfunction *hfunc, CUmodule hmod, const char *name)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuModuleLoad: module with: fname 	&lt;C: CUresult __stdcall cuModuleLoad(CUmodule *module, const char *fname)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cuModuleLoadDataEx: module with: image with: numOptions with: options with: optionValues	&lt;C: CUresult __stdcall cuModuleLoadDataEx(CUmodule *module, const void *image, unsigned int numOptions, CUjit_option *options, void **optionValues)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cudaDeviceSynchronize	&lt;C:cudaError_t __stdcall cudaDeviceSynchronize(void)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cudaFree: dptr	&lt;C:cudaError_t __stdcall cudaFree(void* dptr)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cudaMallocManaged: devPtr size: size flags: flags	&lt;C:cudaError_t __stdcall cudaMallocManaged(void **devPtr, size_t size, unsigned int flags)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">cudaSetDevice: device	&lt;C:cudaError_t __stdcall cudaSetDevice(int  device)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcCompileProgram: prog numOptions: numOptions options: options	&lt;C:nvrtcResult __stdcall nvrtcCompileProgram(nvrtcProgram prog, int numOptions, const char * const *options)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcCreateProgram: prog src: src name: name numHeaders: numHeaders headers: headers includeNamesv: includeNamesv	&lt;C:nvrtcResult __stdcall nvrtcCreateProgram(nvrtcProgram *prog,                               const char *src,                               const char *name,                               int numHeaders,                               const char * const *headers,                               const char * const *includeNamesv)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcDestroyProgram: prog	&lt;C:nvrtcResult __stdcall nvrtcDestroyProgram(nvrtcProgram * prog)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcGetPTX: prog ptx: ptx	&lt;C:nvrtcResult __stdcall nvrtcGetPTX(nvrtcProgram prog, char *ptx)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcGetPTXSize: prog ptxSizeRet: ptxSizeRet	&lt;C:nvrtcResult __stdcall nvrtcGetPTXSize(nvrtcProgram prog, size_t *ptxSizeRet)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcGetProgramLog: prog log: log	&lt;C:nvrtcResult __stdcall nvrtcGetProgramLog(nvrtcProgram prog, char *log)&gt;	^self externalAccessFailedWith: _errorCode</body><body package="StCUDA">nvrtcGetProgramLogSize: prog logSizeRet: logSizeRet	&lt;C:nvrtcResult __stdcall nvrtcGetProgramLogSize(nvrtcProgram prog, size_t *logSizeRet)&gt;	^self externalAccessFailedWith: _errorCode</body></methods><methods><class-id>StCUDAWin64</class-id> <category>enums</category><body package="StCUDA">CUdevice_attribute_enum	&lt;C:enum CUdevice_attribute_enum {    CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK = 1,              /**&lt; Maximum number of threads per block */    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X = 2,                    /**&lt; Maximum block dimension X */    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y = 3,                    /**&lt; Maximum block dimension Y */    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z = 4,                    /**&lt; Maximum block dimension Z */    CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X = 5,                     /**&lt; Maximum grid dimension X */    CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y = 6,                     /**&lt; Maximum grid dimension Y */    CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z = 7,                     /**&lt; Maximum grid dimension Z */    CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK = 8,        /**&lt; Maximum shared memory available per block in bytes */    CU_DEVICE_ATTRIBUTE_SHARED_MEMORY_PER_BLOCK = 8,            /**&lt; Deprecated, use CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK */    CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY = 9,              /**&lt; Memory available on device for __constant__ variables in a CUDA C kernel in bytes */    CU_DEVICE_ATTRIBUTE_WARP_SIZE = 10,                         /**&lt; Warp size in threads */    CU_DEVICE_ATTRIBUTE_MAX_PITCH = 11,                         /**&lt; Maximum pitch in bytes allowed by memory copies */    CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK = 12,           /**&lt; Maximum number of 32-bit registers available per block */    CU_DEVICE_ATTRIBUTE_REGISTERS_PER_BLOCK = 12,               /**&lt; Deprecated, use CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK */    CU_DEVICE_ATTRIBUTE_CLOCK_RATE = 13,                        /**&lt; Typical clock frequency in kilohertz */    CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT = 14,                 /**&lt; Alignment requirement for textures */    CU_DEVICE_ATTRIBUTE_GPU_OVERLAP = 15,                       /**&lt; Device can possibly copy memory and execute a kernel concurrently. Deprecated. Use instead CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT. */    CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT = 16,              /**&lt; Number of multiprocessors on device */    CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT = 17,               /**&lt; Specifies whether there is a run time limit on kernels */    CU_DEVICE_ATTRIBUTE_INTEGRATED = 18,                        /**&lt; Device is integrated with host memory */    CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY = 19,               /**&lt; Device can map host memory into CUDA address space */    CU_DEVICE_ATTRIBUTE_COMPUTE_MODE = 20,                      /**&lt; Compute mode (See ::CUcomputemode for details) */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH = 21,           /**&lt; Maximum 1D texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_WIDTH = 22,           /**&lt; Maximum 2D texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_HEIGHT = 23,          /**&lt; Maximum 2D texture height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH = 24,           /**&lt; Maximum 3D texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT = 25,          /**&lt; Maximum 3D texture height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH = 26,           /**&lt; Maximum 3D texture depth */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH = 27,   /**&lt; Maximum 2D layered texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT = 28,  /**&lt; Maximum 2D layered texture height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS = 29,  /**&lt; Maximum layers in a 2D layered texture */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_WIDTH = 27,     /**&lt; Deprecated, use CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_HEIGHT = 28,    /**&lt; Deprecated, use CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES = 29, /**&lt; Deprecated, use CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS */    CU_DEVICE_ATTRIBUTE_SURFACE_ALIGNMENT = 30,                 /**&lt; Alignment requirement for surfaces */    CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS = 31,                /**&lt; Device can possibly execute multiple kernels concurrently */    CU_DEVICE_ATTRIBUTE_ECC_ENABLED = 32,                       /**&lt; Device has ECC support enabled */    CU_DEVICE_ATTRIBUTE_PCI_BUS_ID = 33,                        /**&lt; PCI bus ID of the device */    CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID = 34,                     /**&lt; PCI device ID of the device */    CU_DEVICE_ATTRIBUTE_TCC_DRIVER = 35,                        /**&lt; Device is using TCC driver model */    CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE = 36,                 /**&lt; Peak memory clock frequency in kilohertz */    CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH = 37,           /**&lt; Global memory bus width in bits */    CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE = 38,                     /**&lt; Size of L2 cache in bytes */    CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR = 39,    /**&lt; Maximum resident threads per multiprocessor */    CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT = 40,                /**&lt; Number of asynchronous engines */    CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING = 41,                /**&lt; Device shares a unified address space with the host */        CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH = 42,   /**&lt; Maximum 1D layered texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS = 43,  /**&lt; Maximum layers in a 1D layered texture */    CU_DEVICE_ATTRIBUTE_CAN_TEX2D_GATHER = 44,                  /**&lt; Deprecated, do not use. */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_WIDTH = 45,    /**&lt; Maximum 2D texture width if CUDA_ARRAY3D_TEXTURE_GATHER is set */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_HEIGHT = 46,   /**&lt; Maximum 2D texture height if CUDA_ARRAY3D_TEXTURE_GATHER is set */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE = 47, /**&lt; Alternate maximum 3D texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE = 48,/**&lt; Alternate maximum 3D texture height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE = 49, /**&lt; Alternate maximum 3D texture depth */    CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID = 50,                     /**&lt; PCI domain ID of the device */    CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT = 51,           /**&lt; Pitch alignment requirement for textures */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_WIDTH = 52,      /**&lt; Maximum cubemap texture width/height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH = 53,  /**&lt; Maximum cubemap layered texture width/height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS = 54, /**&lt; Maximum layers in a cubemap layered texture */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_WIDTH = 55,           /**&lt; Maximum 1D surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_WIDTH = 56,           /**&lt; Maximum 2D surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_HEIGHT = 57,          /**&lt; Maximum 2D surface height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_WIDTH = 58,           /**&lt; Maximum 3D surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_HEIGHT = 59,          /**&lt; Maximum 3D surface height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_DEPTH = 60,           /**&lt; Maximum 3D surface depth */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_WIDTH = 61,   /**&lt; Maximum 1D layered surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_LAYERS = 62,  /**&lt; Maximum layers in a 1D layered surface */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_WIDTH = 63,   /**&lt; Maximum 2D layered surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_HEIGHT = 64,  /**&lt; Maximum 2D layered surface height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_LAYERS = 65,  /**&lt; Maximum layers in a 2D layered surface */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_WIDTH = 66,      /**&lt; Maximum cubemap surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH = 67,  /**&lt; Maximum cubemap layered surface width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS = 68, /**&lt; Maximum layers in a cubemap layered surface */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH = 69,    /**&lt; Maximum 1D linear texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH = 70,    /**&lt; Maximum 2D linear texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT = 71,   /**&lt; Maximum 2D linear texture height */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH = 72,    /**&lt; Maximum 2D linear texture pitch in bytes */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH = 73, /**&lt; Maximum mipmapped 2D texture width */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT = 74,/**&lt; Maximum mipmapped 2D texture height */    CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR = 75,          /**&lt; Major compute capability version number */         CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR = 76,          /**&lt; Minor compute capability version number */    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH = 77, /**&lt; Maximum mipmapped 1D texture width */    CU_DEVICE_ATTRIBUTE_STREAM_PRIORITIES_SUPPORTED = 78,       /**&lt; Device supports stream priorities */    CU_DEVICE_ATTRIBUTE_GLOBAL_L1_CACHE_SUPPORTED = 79,         /**&lt; Device supports caching globals in L1 */    CU_DEVICE_ATTRIBUTE_LOCAL_L1_CACHE_SUPPORTED = 80,          /**&lt; Device supports caching locals in L1 */    CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR = 81,  /**&lt; Maximum shared memory available per multiprocessor in bytes */    CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR = 82,  /**&lt; Maximum number of 32-bit registers available per multiprocessor */    CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY = 83,                    /**&lt; Device can allocate managed memory on this system */    CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD = 84,                    /**&lt; Device is on a multi-GPU board */     CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD_GROUP_ID = 85,           /**&lt; Unique id for a group of devices on the same multi-GPU board */    CU_DEVICE_ATTRIBUTE_HOST_NATIVE_ATOMIC_SUPPORTED = 86,       /**&lt; Link between the device and the host supports native atomic operations (this is a placeholder attribute, and is not supported on any current hardware)*/    CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO = 87,  /**&lt; Ratio of single precision performance (in floating-point operations per second) to double precision performance */    CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS = 88,            /**&lt; Device supports coherently accessing pageable memory without calling cudaHostRegister on it */    CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS = 89,         /**&lt; Device can coherently access managed memory concurrently with the CPU */    CU_DEVICE_ATTRIBUTE_COMPUTE_PREEMPTION_SUPPORTED = 90,      /**&lt; Device supports compute preemption. */    CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM = 91, /**&lt; Device can access host registered memory at the same virtual address as the CPU */    CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_MEM_OPS = 92,            /**&lt; ::cuStreamBatchMemOp and related APIs are supported. */    CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS = 93,     /**&lt; 64-bit operations are supported in ::cuStreamBatchMemOp and related APIs. */    CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR = 94,     /**&lt; ::CU_STREAM_WAIT_VALUE_NOR is supported. */    CU_DEVICE_ATTRIBUTE_COOPERATIVE_LAUNCH = 95,                /**&lt; Device supports launching cooperative kernels via ::cuLaunchCooperativeKernel */    CU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH = 96,   /**&lt; Device can participate in cooperative kernels launched via ::cuLaunchCooperativeKernelMultiDevice */    CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN = 97, /**&lt; Maximum optin shared memory per block */    CU_DEVICE_ATTRIBUTE_MAX}&gt;</body><body package="StCUDA">CUjit_option_enum	&lt;C: enum CUjit_option_enum {    /**     * Max number of registers that a thread may use.\n     * Option type: unsigned int\n     * Applies to: compiler only     */    CU_JIT_MAX_REGISTERS = 0,    /**     * IN: Specifies minimum number of threads per block to target compilation     * for\n     * OUT: Returns the number of threads the compiler actually targeted.     * This restricts the resource utilization fo the compiler (e.g. max     * registers) such that a block with the given number of threads should be     * able to launch based on register limitations. Note, this option does not     * currently take into account any other resource limitations, such as     * shared memory utilization.\n     * Cannot be combined with ::CU_JIT_TARGET.\n     * Option type: unsigned int\n     * Applies to: compiler only     */    CU_JIT_THREADS_PER_BLOCK,    /**     * Overwrites the option value with the total wall clock time, in     * milliseconds, spent in the compiler and linker\n     * Option type: float\n     * Applies to: compiler and linker     */    CU_JIT_WALL_TIME,    /**     * Pointer to a buffer in which to print any log messages     * that are informational in nature (the buffer size is specified via     * option ::CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES)\n     * Option type: char *\n     * Applies to: compiler and linker     */    CU_JIT_INFO_LOG_BUFFER,    /**     * IN: Log buffer size in bytes.  Log messages will be capped at this size     * (including null terminator)\n     * OUT: Amount of log buffer filled with messages\n     * Option type: unsigned int\n     * Applies to: compiler and linker     */    CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES,    /**     * Pointer to a buffer in which to print any log messages that     * reflect errors (the buffer size is specified via option     * ::CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES)\n     * Option type: char *\n     * Applies to: compiler and linker     */    CU_JIT_ERROR_LOG_BUFFER,    /**     * IN: Log buffer size in bytes.  Log messages will be capped at this size     * (including null terminator)\n     * OUT: Amount of log buffer filled with messages\n     * Option type: unsigned int\n     * Applies to: compiler and linker     */    CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES,    /**     * Level of optimizations to apply to generated code (0 - 4), with 4     * being the default and highest level of optimizations.\n     * Option type: unsigned int\n     * Applies to: compiler only     */    CU_JIT_OPTIMIZATION_LEVEL,    /**     * No option value required. Determines the target based on the current     * attached context (default)\n     * Option type: No option value needed\n     * Applies to: compiler and linker     */    CU_JIT_TARGET_FROM_CUCONTEXT,    /**     * Target is chosen based on supplied ::CUjit_target.  Cannot be     * combined with ::CU_JIT_THREADS_PER_BLOCK.\n     * Option type: unsigned int for enumerated type ::CUjit_target\n     * Applies to: compiler and linker     */    CU_JIT_TARGET,    /**     * Specifies choice of fallback strategy if matching cubin is not found.     * Choice is based on supplied ::CUjit_fallback.  This option cannot be     * used with cuLink* APIs as the linker requires exact matches.\n     * Option type: unsigned int for enumerated type ::CUjit_fallback\n     * Applies to: compiler only     */    CU_JIT_FALLBACK_STRATEGY,    /**     * Specifies whether to create debug information in output (-g)     * (0: false, default)\n     * Option type: int\n     * Applies to: compiler and linker     */    CU_JIT_GENERATE_DEBUG_INFO,    /**     * Generate verbose log messages (0: false, default)\n     * Option type: int\n     * Applies to: compiler and linker     */    CU_JIT_LOG_VERBOSE,    /**     * Generate line number information (-lineinfo) (0: false, default)\n     * Option type: int\n     * Applies to: compiler only     */    CU_JIT_GENERATE_LINE_INFO,    /**     * Specifies whether to enable caching explicitly (-dlcm) \n     * Choice is based on supplied ::CUjit_cacheMode_enum.\n     * Option type: unsigned int for enumerated type ::CUjit_cacheMode_enum\n     * Applies to: compiler only     */    CU_JIT_CACHE_MODE,    /**     * The below jit options are used for internal purposes only, in this version of CUDA     */    CU_JIT_NEW_SM3X_OPT,    CU_JIT_FAST_COMPILE,    CU_JIT_NUM_OPTIONS	}&gt;</body><body package="StCUDA">cudaError_enum	&lt;C: enum cudaError_enum {    /**     * The API call returned with no errors. In the case of query calls, this     * can also mean that the operation being queried is complete (see     * ::cuEventQuery() and ::cuStreamQuery()).     */    CUDA_SUCCESS                              = 0,    /**     * This indicates that one or more of the parameters passed to the API call     * is not within an acceptable range of values.     */    CUDA_ERROR_INVALID_VALUE                  = 1,    /**     * The API call failed because it was unable to allocate enough memory to     * perform the requested operation.     */    CUDA_ERROR_OUT_OF_MEMORY                  = 2,    /**     * This indicates that the CUDA driver has not been initialized with     * ::cuInit() or that initialization has failed.     */    CUDA_ERROR_NOT_INITIALIZED                = 3,    /**     * This indicates that the CUDA driver is in the process of shutting down.     */    CUDA_ERROR_DEINITIALIZED                  = 4,    /**     * This indicates profiler is not initialized for this run. This can     * happen when the application is running with external profiling tools     * like visual profiler.     */    CUDA_ERROR_PROFILER_DISABLED              = 5,    /**     * \deprecated     * This error return is deprecated as of CUDA 5.0. It is no longer an error     * to attempt to enable/disable the profiling via ::cuProfilerStart or     * ::cuProfilerStop without initialization.     */    CUDA_ERROR_PROFILER_NOT_INITIALIZED       = 6,    /**     * \deprecated     * This error return is deprecated as of CUDA 5.0. It is no longer an error     * to call cuProfilerStart() when profiling is already enabled.     */    CUDA_ERROR_PROFILER_ALREADY_STARTED       = 7,    /**     * \deprecated     * This error return is deprecated as of CUDA 5.0. It is no longer an error     * to call cuProfilerStop() when profiling is already disabled.     */    CUDA_ERROR_PROFILER_ALREADY_STOPPED       = 8,    /**     * This indicates that no CUDA-capable devices were detected by the installed     * CUDA driver.     */    CUDA_ERROR_NO_DEVICE                      = 100,    /**     * This indicates that the device ordinal supplied by the user does not     * correspond to a valid CUDA device.     */    CUDA_ERROR_INVALID_DEVICE                 = 101,    /**     * This indicates that the device kernel image is invalid. This can also     * indicate an invalid CUDA module.     */    CUDA_ERROR_INVALID_IMAGE                  = 200,    /**     * This most frequently indicates that there is no context bound to the     * current thread. This can also be returned if the context passed to an     * API call is not a valid handle (such as a context that has had     * ::cuCtxDestroy() invoked on it). This can also be returned if a user     * mixes different API versions (i.e. 3010 context with 3020 API calls).     * See ::cuCtxGetApiVersion() for more details.     */    CUDA_ERROR_INVALID_CONTEXT                = 201,    /**     * This indicated that the context being supplied as a parameter to the     * API call was already the active context.     * \deprecated     * This error return is deprecated as of CUDA 3.2. It is no longer an     * error to attempt to push the active context via ::cuCtxPushCurrent().     */    CUDA_ERROR_CONTEXT_ALREADY_CURRENT        = 202,    /**     * This indicates that a map or register operation has failed.     */    CUDA_ERROR_MAP_FAILED                     = 205,    /**     * This indicates that an unmap or unregister operation has failed.     */    CUDA_ERROR_UNMAP_FAILED                   = 206,    /**     * This indicates that the specified array is currently mapped and thus     * cannot be destroyed.     */    CUDA_ERROR_ARRAY_IS_MAPPED                = 207,    /**     * This indicates that the resource is already mapped.     */    CUDA_ERROR_ALREADY_MAPPED                 = 208,    /**     * This indicates that there is no kernel image available that is suitable     * for the device. This can occur when a user specifies code generation     * options for a particular CUDA source file that do not include the     * corresponding device configuration.     */    CUDA_ERROR_NO_BINARY_FOR_GPU              = 209,    /**     * This indicates that a resource has already been acquired.     */    CUDA_ERROR_ALREADY_ACQUIRED               = 210,    /**     * This indicates that a resource is not mapped.     */    CUDA_ERROR_NOT_MAPPED                     = 211,    /**     * This indicates that a mapped resource is not available for access as an     * array.     */    CUDA_ERROR_NOT_MAPPED_AS_ARRAY            = 212,    /**     * This indicates that a mapped resource is not available for access as a     * pointer.     */    CUDA_ERROR_NOT_MAPPED_AS_POINTER          = 213,    /**     * This indicates that an uncorrectable ECC error was detected during     * execution.     */    CUDA_ERROR_ECC_UNCORRECTABLE              = 214,    /**     * This indicates that the ::CUlimit passed to the API call is not     * supported by the active device.     */    CUDA_ERROR_UNSUPPORTED_LIMIT              = 215,    /**     * This indicates that the ::CUcontext passed to the API call can     * only be bound to a single CPU thread at a time but is already      * bound to a CPU thread.     */    CUDA_ERROR_CONTEXT_ALREADY_IN_USE         = 216,    /**     * This indicates that peer access is not supported across the given     * devices.     */    CUDA_ERROR_PEER_ACCESS_UNSUPPORTED        = 217,    /**     * This indicates that a PTX JIT compilation failed.     */    CUDA_ERROR_INVALID_PTX                    = 218,    /**     * This indicates an error with OpenGL or DirectX context.     */    CUDA_ERROR_INVALID_GRAPHICS_CONTEXT       = 219,    /**    * This indicates that an uncorrectable NVLink error was detected during the    * execution.    */    CUDA_ERROR_NVLINK_UNCORRECTABLE           = 220,    /**     * This indicates that the device kernel source is invalid.     */    CUDA_ERROR_INVALID_SOURCE                 = 300,    /**     * This indicates that the file specified was not found.     */    CUDA_ERROR_FILE_NOT_FOUND                 = 301,    /**     * This indicates that a link to a shared object failed to resolve.     */    CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND = 302,    /**     * This indicates that initialization of a shared object failed.     */    CUDA_ERROR_SHARED_OBJECT_INIT_FAILED      = 303,    /**     * This indicates that an OS call failed.     */    CUDA_ERROR_OPERATING_SYSTEM               = 304,    /**     * This indicates that a resource handle passed to the API call was not     * valid. Resource handles are opaque types like ::CUstream and ::CUevent.     */    CUDA_ERROR_INVALID_HANDLE                 = 400,    /**     * This indicates that a named symbol was not found. Examples of symbols     * are global/constant variable names, texture names, and surface names.     */    CUDA_ERROR_NOT_FOUND                      = 500,    /**     * This indicates that asynchronous operations issued previously have not     * completed yet. This result is not actually an error, but must be indicated     * differently than ::CUDA_SUCCESS (which indicates completion). Calls that     * may return this value include ::cuEventQuery() and ::cuStreamQuery().     */    CUDA_ERROR_NOT_READY                      = 600,    /**     * While executing a kernel, the device encountered a     * load or store instruction on an invalid memory address.     * The context cannot be used, so it must be destroyed (and a new one should be created).     * All existing device memory allocations from this context are invalid     * and must be reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_ILLEGAL_ADDRESS                = 700,    /**     * This indicates that a launch did not occur because it did not have     * appropriate resources. This error usually indicates that the user has     * attempted to pass too many arguments to the device kernel, or the     * kernel launch specifies too many threads for the kernel's register     * count. Passing arguments of the wrong size (i.e. a 64-bit pointer     * when a 32-bit int is expected) is equivalent to passing too many     * arguments and can also result in this error.     */    CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES        = 701,    /**     * This indicates that the device kernel took too long to execute. This can     * only occur if timeouts are enabled - see the device attribute     * ::CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT for more information. The     * context cannot be used (and must be destroyed similar to     * ::CUDA_ERROR_LAUNCH_FAILED). All existing device memory allocations from     * this context are invalid and must be reconstructed if the program is to     * continue using CUDA.     */    CUDA_ERROR_LAUNCH_TIMEOUT                 = 702,    /**     * This error indicates a kernel launch that uses an incompatible texturing     * mode.     */    CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING  = 703,        /**     * This error indicates that a call to ::cuCtxEnablePeerAccess() is     * trying to re-enable peer access to a context which has already     * had peer access to it enabled.     */    CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED    = 704,    /**     * This error indicates that ::cuCtxDisablePeerAccess() is      * trying to disable peer access which has not been enabled yet      * via ::cuCtxEnablePeerAccess().      */    CUDA_ERROR_PEER_ACCESS_NOT_ENABLED        = 705,    /**     * This error indicates that the primary context for the specified device     * has already been initialized.     */    CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE         = 708,    /**     * This error indicates that the context current to the calling thread     * has been destroyed using ::cuCtxDestroy, or is a primary context which     * has not yet been initialized.     */    CUDA_ERROR_CONTEXT_IS_DESTROYED           = 709,    /**     * A device-side assert triggered during kernel execution. The context     * cannot be used anymore, and must be destroyed. All existing device      * memory allocations from this context are invalid and must be      * reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_ASSERT                         = 710,    /**     * This error indicates that the hardware resources required to enable     * peer access have been exhausted for one or more of the devices      * passed to ::cuCtxEnablePeerAccess().     */    CUDA_ERROR_TOO_MANY_PEERS                 = 711,    /**     * This error indicates that the memory range passed to ::cuMemHostRegister()     * has already been registered.     */    CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED = 712,    /**     * This error indicates that the pointer passed to ::cuMemHostUnregister()     * does not correspond to any currently registered memory region.     */    CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED     = 713,    /**     * While executing a kernel, the device encountered a stack error.     * This can be due to stack corruption or exceeding the stack size limit.     * The context cannot be used, so it must be destroyed (and a new one should be created).     * All existing device memory allocations from this context are invalid     * and must be reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_HARDWARE_STACK_ERROR           = 714,    /**     * While executing a kernel, the device encountered an illegal instruction.     * The context cannot be used, so it must be destroyed (and a new one should be created).     * All existing device memory allocations from this context are invalid     * and must be reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_ILLEGAL_INSTRUCTION            = 715,    /**     * While executing a kernel, the device encountered a load or store instruction     * on a memory address which is not aligned.     * The context cannot be used, so it must be destroyed (and a new one should be created).     * All existing device memory allocations from this context are invalid     * and must be reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_MISALIGNED_ADDRESS             = 716,    /**     * While executing a kernel, the device encountered an instruction     * which can only operate on memory locations in certain address spaces     * (global, shared, or local), but was supplied a memory address not     * belonging to an allowed address space.     * The context cannot be used, so it must be destroyed (and a new one should be created).     * All existing device memory allocations from this context are invalid     * and must be reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_INVALID_ADDRESS_SPACE          = 717,    /**     * While executing a kernel, the device program counter wrapped its address space.     * The context cannot be used, so it must be destroyed (and a new one should be created).     * All existing device memory allocations from this context are invalid     * and must be reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_INVALID_PC                     = 718,    /**     * An exception occurred on the device while executing a kernel. Common     * causes include dereferencing an invalid device pointer and accessing     * out of bounds shared memory. The context cannot be used, so it must     * be destroyed (and a new one should be created). All existing device     * memory allocations from this context are invalid and must be     * reconstructed if the program is to continue using CUDA.     */    CUDA_ERROR_LAUNCH_FAILED                  = 719,    /**     * This error indicates that the attempted operation is not permitted.     */    CUDA_ERROR_NOT_PERMITTED                  = 800,    /**     * This error indicates that the attempted operation is not supported     * on the current system or device.     */    CUDA_ERROR_NOT_SUPPORTED                  = 801,    /**     * This indicates that an unknown internal error has occurred.     */    CUDA_ERROR_UNKNOWN                        = 999	}&gt;</body><body package="StCUDA">nvrtcResult_enum	&lt;C:enum nvrtcResult_enum {  NVRTC_SUCCESS = 0,  NVRTC_ERROR_OUT_OF_MEMORY = 1,  NVRTC_ERROR_PROGRAM_CREATION_FAILURE = 2,  NVRTC_ERROR_INVALID_INPUT = 3,  NVRTC_ERROR_INVALID_PROGRAM = 4,  NVRTC_ERROR_INVALID_OPTION = 5,  NVRTC_ERROR_COMPILATION = 6,  NVRTC_ERROR_BUILTIN_OPERATION_FAILURE = 7,  NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION = 8,  NVRTC_ERROR_NO_LOWERED_NAMES_BEFORE_COMPILATION = 9,  NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID = 10,  NVRTC_ERROR_INTERNAL_ERROR = 11}&gt;</body></methods><methods><class-id>StCUDAWin64</class-id> <category>structs</category><body package="StCUDA">CUctx_st	&lt;C: struct CUctx_st&gt;</body><body package="StCUDA">CUfunc_st	&lt;C: struct CUfunc_st&gt;</body><body package="StCUDA">CUmod_st	&lt;C: struct CUmod_st&gt;</body><body package="StCUDA">CUstream_st	&lt;C: struct CUstream_st&gt;</body><body package="StCUDA">_nvrtcProgram	&lt;C:struct _nvrtcProgram&gt;</body></methods><methods><class-id>StCUDAWin64</class-id> <category>types</category><body package="StCUDA">CUcontext	&lt;C: typedef void * CUcontext&gt;</body><body package="StCUDA">CUdevice	&lt;C: typedef int CUdevice&gt;</body><body package="StCUDA">CUdeviceptr	&lt;C:typedef unsigned long long CUdeviceptr&gt;</body><body package="StCUDA">CUfunction	&lt;C: typedef void * CUfunction&gt;</body><body package="StCUDA">CUjit_option	&lt;C: typedef enum CUjit_option_enum CUjit_option&gt;</body><body package="StCUDA">CUmodule	&lt;C: typedef void * CUmodule&gt;</body><body package="StCUDA">CUresult	&lt;C: typedef enum cudaError_enum CUresult&gt;</body><body package="StCUDA">CUstream	&lt;C: typedef void * CUstream&gt;</body><body package="StCUDA">cudaError_t	&lt;C:typedef enum cudaError cudaError_t&gt;</body><body package="StCUDA">nvrtcProgram	&lt;C:typedef struct _nvrtcProgram * nvrtcProgram&gt;</body><body package="StCUDA">nvrtcResult	&lt;C:typedef enum nvrtcResult_enum nvrtcResult&gt;</body><body package="StCUDA">size_t	&lt;C: typedef unsigned __int64 size_t&gt;</body></methods><methods><class-id>StCUDA</class-id> <category>helpers</category><body package="StCUDA">compileProgram	| fileContents extension cuSourcePtr prog_ptr numCompileOptions compileParams_ptrptr option logSizePtr logPtr ptxSizePtr |	Transcript show: ('&lt;n&gt;kernel file = &lt;1s&gt;' expandMacrosWith: ptxOrcuFile).	fileContents := ptxOrcuFile asFilename readStream binary contents				copyWith: 0.	extension := ptxOrcuFile asFilename extension.	extension = '.ptx'		ifTrue: [ptxSourcePtr := fileContents gcCopyToHeap]		ifFalse: 			[extension = '.cu'				ifTrue: 					[cuSourcePtr := fileContents gcCopyToHeap.					prog_ptr := ICUDA nvrtcProgram gcMalloc.					ICUDA checkNvrtcErrors: 							[ICUDA								nvrtcCreateProgram: prog_ptr								src: cuSourcePtr								name: ptxOrcuFile gcCopyToHeap								numHeaders: 0								headers: nil								includeNamesv: nil].										[numCompileOptions := compileOptions size.					compileParams_ptrptr := CIntegerType char pointerType								gcMalloc: (numCompileOptions max: 1).					1 to: numCompileOptions						do: 							[:i |							option := (compileOptions at: i) copyWith: 0 asCharacter.							compileParams_ptrptr at: i - 1 put: option gcCopyToHeap].					ICUDA checkNvrtcErrors: 							[ICUDA								nvrtcCompileProgram: prog_ptr contents								numOptions: numCompileOptions								options: compileParams_ptrptr].					logSizePtr := ICUDA size_t gcMalloc.					ICUDA checkNvrtcErrors: 							[ICUDA nvrtcGetProgramLogSize: prog_ptr contents logSizeRet: logSizePtr].					logPtr := CIntegerType char gcMalloc: logSizePtr contents + 1.					ICUDA						checkNvrtcErrors: [ICUDA nvrtcGetProgramLog: prog_ptr contents log: logPtr].					logPtr at: logSizePtr contents put: 0.					Transcript show: ('&lt;n&gt;NVRTC Compile Log = &lt;1s&gt;'								expandMacrosWith: logPtr copyCStringFromHeap).					ptxSizePtr := ICUDA size_t gcMalloc.					ICUDA checkNvrtcErrors: 							[ICUDA nvrtcGetPTXSize: prog_ptr contents ptxSizeRet: ptxSizePtr].					ptxSourcePtr := CIntegerType char gcMalloc: ptxSizePtr contents.					ICUDA						checkNvrtcErrors: [ICUDA nvrtcGetPTX: prog_ptr contents ptx: ptxSourcePtr]]							ensure: [ICUDA checkNvrtcErrors: [ICUDA nvrtcDestroyProgram: prog_ptr]]]				ifFalse: [self halt: 'Check this.']]</body><body package="StCUDA">createCUDAContext	ICUDA checkCudaErrors: 			[ICUDA cuDeviceComputeCapability: majorPtr with: minorPtr with: bestDevID].	Transcript		show: ('&lt;n&gt;major, minor = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: majorPtr contents				with: minorPtr contents).	cuDevicePtr := ICUDA CUdevice gcCalloc.	ICUDA checkCudaErrors: [ICUDA cuDeviceGet: cuDevicePtr with: bestDevID].	deviceNamePtr := CIntegerType char gcCalloc: 256.	ICUDA checkCudaErrors: 			[ICUDA				cuDeviceGetName: deviceNamePtr				with: 256				with: cuDevicePtr contents].	Transcript show: ('&lt;n&gt;deviceName = &lt;1p&gt;'				expandMacrosWith: deviceNamePtr copyCStringFromHeap).	deviceTotalMemPtr := ICUDA size_t gcCalloc.	ICUDA checkCudaErrors: 			[ICUDA cuDeviceTotalMem: deviceTotalMemPtr with: cuDevicePtr contents].	Transcript show: ('&lt;n&gt;Total amount of global memory = &lt;1p&gt;'				expandMacrosWith: deviceTotalMemPtr contents).	cuContextPtr := ICUDA CUcontext gcCalloc.	ICUDA checkCudaErrors: 			[ICUDA				cuCtxCreate: cuContextPtr				with: 0				with: cuDevicePtr contents]</body><body package="StCUDA">createFunction	"Must call StCUDA&gt;&gt;createModule first."	functionNamePtr := functionName gcCopyToHeap.	cuFunctionPtr := ICUDA CUfunction gcCalloc.	ICUDA checkCudaErrors: 			[ICUDA				cuModuleGetFunction: cuFunctionPtr				with: cuModulePtr contents				with: functionNamePtr]</body><body package="StCUDA">createModule	| jitNumOptions jitOptionsPtr jitOptValsPtrPtr |	cuModulePtr := ICUDA CUmodule gcCalloc.	jitNumOptions := jitOptions size.	jitOptionsPtr := ICUDA CUjit_option_enum gcCalloc: jitNumOptions.	jitOptValsPtrPtr := CVoidType void pointerType gcCalloc: jitNumOptions.	jitOptionsPtr at: 0 put: ICUDA CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES.	jitOptValsPtrPtr at: 0		put: (jitOptions at: #CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES).	jitOptionsPtr at: 1 put: ICUDA CU_JIT_INFO_LOG_BUFFER.	jitOptValsPtrPtr at: 1 put: (jitOptions at: #CU_JIT_INFO_LOG_BUFFER).	jitOptionsPtr at: 2 put: ICUDA CU_JIT_MAX_REGISTERS.	jitOptValsPtrPtr at: 2 put: (jitOptions at: #CU_JIT_MAX_REGISTERS).	ICUDA checkCudaErrors: 			[ICUDA				cuModuleLoadDataEx: cuModulePtr				with: ptxSourcePtr				with: jitNumOptions				with: jitOptionsPtr				with: jitOptValsPtrPtr].	Transcript show: ('&lt;n&gt;CU_JIT_INFO_LOG_BUFFER = &lt;1s&gt;'				expandMacrosWith: (jitOptions at: #CU_JIT_INFO_LOG_BUFFER)						copyCStringFromHeap)</body><body package="StCUDA">destroyCUDAContext	^ICUDA checkCudaErrors: [ICUDA cuCtxDestroy: cuContextPtr contents]</body><body package="StCUDA">findBestDevID	| nCore computeModePtr clockRatePtr flops multiProcessorCountPtr |	deviceCountPtr := CIntegerType int gcCalloc.	ICUDA checkCudaErrors: [ICUDA cuDeviceGetCount: deviceCountPtr].	deviceCountPtr contents isZero ifTrue: [self halt].	maxMajor := 0.	majorPtr := CIntegerType int gcCalloc.	minorPtr := CIntegerType int gcCalloc.	1 to: deviceCountPtr contents		do: 			[:i |			devID := i - 1.			ICUDA checkCudaErrors: 					[ICUDA cuDeviceComputeCapability: majorPtr with: minorPtr with: devID].			maxMajor := maxMajor max: majorPtr contents].	multiProcessorCountPtr := CIntegerType int gcCalloc.	clockRatePtr := CIntegerType int gcCalloc.	computeModePtr := CIntegerType int gcCalloc.	maxFlops := 0.	bestDevID := 0.	1 to: deviceCountPtr contents		do: 			[:i |			devID := i - 1.			ICUDA checkCudaErrors: 					[ICUDA						cuDeviceGetAttribute: computeModePtr						with: ICUDA CU_DEVICE_ATTRIBUTE_COMPUTE_MODE						with: devID].			computeModePtr contents = ICUDA CU_COMPUTEMODE_PROHIBITED				ifTrue: [self halt]				ifFalse: 					[ICUDA checkCudaErrors: 							[ICUDA cuDeviceComputeCapability: majorPtr with: minorPtr with: devID].					maxMajor = majorPtr contents						ifTrue: 							[nCore := ICUDA _ConvertSMVer2CoresDRV: majorPtr contents										with: minorPtr contents.							ICUDA checkCudaErrors: 									[ICUDA										cuDeviceGetAttribute: multiProcessorCountPtr										with: ICUDA CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT										with: devID].							ICUDA checkCudaErrors: 									[ICUDA										cuDeviceGetAttribute: clockRatePtr										with: ICUDA CU_DEVICE_ATTRIBUTE_CLOCK_RATE										with: devID].							flops := multiProcessorCountPtr contents * nCore * clockRatePtr contents.							maxFlops &lt; flops								ifTrue: 									[maxFlops := flops.									bestDevID := devID]]]]</body><body package="StCUDA">launchKernel	ICUDA checkCudaErrors: 			[ICUDA				cuLaunchKernel: cuFunctionPtr contents				with: gridDimX				with: gridDimY				with: gridDimZ				with: blockDimX				with: blockDimY				with: blockDimZ				with: sharedMemBytes				with: nil				with: kernelParams				with: nil].	ICUDA checkCudaErrors: [ICUDA cuCtxSynchronize]</body></methods><methods><class-id>StCUDA</class-id> <category>ASKOH</category><body package="StCUDA">aDebugTxt	^Filename findDefaultDirectory asString , '\debug.txt'</body><body package="StCUDA">deleteDebugTxt	"self deleteDebugTxt. "	| fn |	fn := self aDebugTxt asFilename.	fn definitelyExists ifTrue: [fn delete]</body><body package="StCUDA">toDebugTxt: aString expandMacrosWith: aObject	"self toDebugTxt: '&lt;n&gt;testing = &lt;1p&gt;' expandMacrosWith: nil. "	self		toFile: self aDebugTxt		string: aString		expandMacrosWith: aObject</body><body package="StCUDA">toDebugTxtShowContextFrom: i to: j	"self toDebugTxtShowContextFrom: 1 to: 10."	| stackStream |	stackStream := WriteStream on: (String new: 400).	stackStream		cr;		cr;		nextPutAll: Timestamp now printString;		nextPutAll: (self stackStringFrom: i to: j).	self toDebugTxtShowString: stackStream contents.</body><body package="StCUDA">toDebugTxtShowObject: aObject	"self toDebugTxtShowObject: 'testing'. "	self toFile: self aDebugTxt showObject: aObject</body><body package="StCUDA">toDebugTxtShowString: aString	"self toDebugTxtShowString: 'testing'. "	self toFile: self aDebugTxt showString: aString</body><body package="StCUDA">toFile: filepath showObject: aObject	"self toFile: 'debug.txt' showObject: 'testing'. "	self toFile: filepath string: '&lt;1p&gt;' expandMacrosWith: aObject</body><body package="StCUDA">toFile: filepath showString: aString	"self toFile: 'debug.txt' showString: 'testing'. "	self toFile: filepath string: '&lt;1s&gt;' expandMacrosWith: aString</body><body package="StCUDA">toFile: filepath string: aString expandMacrosWith: object	"self toFile: 'debug.txt' string: '&lt;n&gt;testing = &lt;1p&gt;' expandMacrosWith: nil."	| stream |	stream := filepath asFilename readAppendStream.	[stream nextPutAll: (aString expandMacrosWith: object)]		ensure: [stream close]</body><body package="StCUDA">toTranscript: aString expandMacrosWith: aObject	"self toTranscript: '&lt;n&gt;testing = &lt;1p&gt;' expandMacrosWith: nil."	Transcript show: (aString expandMacrosWith: aObject)</body></methods><methods><class-id>StCUDA</class-id> <category>examples</category><body package="StCUDA">jitOptionsWithDefaults	| jitLogBufferSize |	jitLogBufferSize := 1024.	jitOptions := Dictionary new.	jitOptions		at: #CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES put: jitLogBufferSize;		at: #CU_JIT_INFO_LOG_BUFFER			put: (CIntegerType char gcCalloc: jitLogBufferSize);		at: #CU_JIT_MAX_REGISTERS put: 32</body></methods><methods><class-id>StCUDA class</class-id> <category>class initialization</category><body package="StCUDA">initialize	ICUDA := OSHandle platformMoniker = #win64				ifTrue: [StCUDAWin64 new]				ifFalse: 					[OSHandle platformMoniker = #linux_x86_64_unix						ifTrue: [StCUDALinux64 new]						ifFalse: [self checkThis]]</body></methods><methods><class-id>StCUDAVectorAdd</class-id> <category>examples</category><body package="StCUDA">exampleVectorAddDrvGC	"self new exampleVectorAddDrvGC."	"C:\ProgramData\NVIDIA Corporation\CUDA Samples\v9.1\0_Simple\vectorAddDrv"	| n byteSize h_A_ptr tempPtr h_B_ptr h_C_ptr cDeviceMemPtr d_A_ptr d_B_ptr d_C_ptr n_ptr answer |	Transcript show: ('&lt;n&gt;&lt;n&gt;&lt;1p&gt;'				expandMacrosWith: 'StCUDAVectorAdd new exampleVectorAddDrvGC.').	ICUDA cuInit0.	self findBestDevID.	self createCUDAContext.		[ptxOrcuFile := '.\StCUDA\vectorAdd_kernel64.ptx'.	compileOptions := Array new.	self compileProgram.	self jitOptionsWithDefaults.	self createModule.	functionName := 'VecAdd_kernel'.	self createFunction.	n := 50000.	byteSize := n * CLimitedPrecisionRealType float sizeof.	Transcript show: ('&lt;n&gt;byteSize = &lt;1p&gt;' expandMacrosWith: byteSize).	h_A_ptr := CLimitedPrecisionRealType float gcMalloc: n.	tempPtr := h_A_ptr copy.	1 to: n		do: 			[:i |			tempPtr contents: i.			tempPtr += 1].	h_B_ptr := CLimitedPrecisionRealType float gcMalloc: n.	tempPtr := h_B_ptr copy.	1 to: n		do: 			[:i |			tempPtr contents: i.			tempPtr += 1].	h_C_ptr := CLimitedPrecisionRealType float gcMalloc: n.	cDeviceMemPtr := OrderedCollection new.	d_A_ptr := ICUDA CUdeviceptr gcMalloc.	cDeviceMemPtr add: d_A_ptr.	ICUDA checkCudaErrors: 			[d_A_ptr contents: 0.			ICUDA cuMemAlloc: d_A_ptr with: byteSize].	d_B_ptr := ICUDA CUdeviceptr gcMalloc.	cDeviceMemPtr add: d_B_ptr.	ICUDA checkCudaErrors: 			[d_B_ptr contents: 0.			ICUDA cuMemAlloc: d_B_ptr with: byteSize].	d_C_ptr := ICUDA CUdeviceptr gcMalloc.	cDeviceMemPtr add: d_C_ptr.	ICUDA checkCudaErrors: 			[d_C_ptr contents: 0.			ICUDA cuMemAlloc: d_C_ptr with: byteSize].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_A_ptr contents				with: h_A_ptr				with: byteSize].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_B_ptr contents				with: h_B_ptr				with: byteSize].	n_ptr := (CIntegerType int gcCalloc)				contents: n;				yourself.	kernelParams := CVoidType void pointerType gcCalloc: 4.	kernelParams		at: 0 put: d_A_ptr;		at: 1 put: d_B_ptr;		at: 2 put: d_C_ptr;		at: 3 put: n_ptr.	blockDimX := 256.	blockDimY := 1.	blockDimZ := 1.	gridDimX := (n + blockDimX - 1) // blockDimX.	gridDimY := 1.	gridDimZ := 1.	sharedMemBytes := 0.	self launchKernel.	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyDtoH: h_C_ptr				with: d_C_ptr contents				with: byteSize].	tempPtr := h_C_ptr copy.	answer := Array new: n.	1 to: n		do: 			[:i |			answer at: i put: tempPtr contents.			tempPtr += 1].	answer inspect.	cDeviceMemPtr		do: [:devicePtr | ICUDA checkCudaErrors: [ICUDA cuMemFree: devicePtr contents]]]			ensure: [self destroyCUDAContext]</body><body package="StCUDA">exampleVectorAddNvrtcGC	"self new exampleVectorAddNvrtcGC."	"C:\ProgramData\NVIDIA Corporation\CUDA Samples\v9.1\0_Simple\vectorAdd_nvrtc"	| n byteSize h_A_ptr tempPtr h_B_ptr h_C_ptr cDeviceMemPtr d_A_ptr d_B_ptr d_C_ptr n_ptr answer |	Transcript show: ('&lt;n&gt;&lt;n&gt;&lt;1p&gt;'				expandMacrosWith: 'StCUDAVectorAdd new exampleVectorAddNvrtcGC.').	ICUDA cuInit0.	self findBestDevID.	self createCUDAContext.		[ptxOrcuFile := '.\StCUDA\vectorAdd_kernel (2).cu'.	compileOptions := Array new.	self compileProgram.	self jitOptionsWithDefaults.	self createModule.	functionName := 'vectorAdd'.	self createFunction.	n := 50000.	byteSize := n * CLimitedPrecisionRealType float sizeof.	Transcript show: ('&lt;n&gt;byteSize = &lt;1p&gt;' expandMacrosWith: byteSize).	h_A_ptr := CLimitedPrecisionRealType float gcMalloc: n.	tempPtr := h_A_ptr copy.	1 to: n		do: 			[:i |			tempPtr contents: i.			tempPtr += 1].	h_B_ptr := CLimitedPrecisionRealType float gcMalloc: n.	tempPtr := h_B_ptr copy.	1 to: n		do: 			[:i |			tempPtr contents: i.			tempPtr += 1].	h_C_ptr := CLimitedPrecisionRealType float gcMalloc: n.	cDeviceMemPtr := OrderedCollection new.	d_A_ptr := ICUDA CUdeviceptr gcMalloc.	cDeviceMemPtr add: d_A_ptr.	ICUDA checkCudaErrors: 			[d_A_ptr contents: 0.			ICUDA cuMemAlloc: d_A_ptr with: byteSize].	d_B_ptr := ICUDA CUdeviceptr gcMalloc.	cDeviceMemPtr add: d_B_ptr.	ICUDA checkCudaErrors: 			[d_B_ptr contents: 0.			ICUDA cuMemAlloc: d_B_ptr with: byteSize].	d_C_ptr := ICUDA CUdeviceptr gcMalloc.	cDeviceMemPtr add: d_C_ptr.	ICUDA checkCudaErrors: 			[d_C_ptr contents: 0.			ICUDA cuMemAlloc: d_C_ptr with: byteSize].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_A_ptr contents				with: h_A_ptr				with: byteSize].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_B_ptr contents				with: h_B_ptr				with: byteSize].	n_ptr := (CIntegerType int gcCalloc)				contents: n;				yourself.	kernelParams := CVoidType void pointerType gcCalloc: 4.	kernelParams		at: 0 put: d_A_ptr;		at: 1 put: d_B_ptr;		at: 2 put: d_C_ptr;		at: 3 put: n_ptr.	blockDimX := 256.	blockDimY := 1.	blockDimZ := 1.	gridDimX := (n + blockDimX - 1) // blockDimX.	gridDimY := 1.	gridDimZ := 1.	sharedMemBytes := 0.	self launchKernel.	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyDtoH: h_C_ptr				with: d_C_ptr contents				with: byteSize].	tempPtr := h_C_ptr copy.	answer := Array new: n.	1 to: n		do: 			[:i |			answer at: i put: tempPtr contents.			tempPtr += 1].	answer inspect.	cDeviceMemPtr		do: [:devicePtr | ICUDA checkCudaErrors: [ICUDA cuMemFree: devicePtr contents]]]			ensure: [self destroyCUDAContext]</body></methods><methods><class-id>StCUDAMatrixMul</class-id> <category>examples</category><body package="StCUDA">exampleMatrixMulDrvGC	"self new exampleMatrixMulDrvGC."	"C:\ProgramData\NVIDIA Corporation\CUDA Samples\v9.1\0_Simple\matrixMulDrv"	"Aij*Bjk = Cik"	| h_A_ptr tempPtr h_B_ptr h_C_ptr d_A_ptr d_B_ptr d_C_ptr cDeviceMemPtr answer block_size mA nA mB nB mC nC sizeOfFloat nA_ptr nB_ptr |	Transcript show: ('&lt;n&gt;&lt;n&gt;&lt;1p&gt;'				expandMacrosWith: 'StCUDAMatrixMul new exampleMatrixMulDrvGC.').	ICUDA cuInit0.	self findBestDevID.	self createCUDAContext.		[ptxOrcuFile := '.\StCUDA\matrixMul_kernel64.ptx'.	compileOptions := Array new.	self compileProgram.	self jitOptionsWithDefaults.	self createModule.	functionName := 'matrixMul_bs32_32bit'.	self createFunction.	block_size := 32.	mA := 6 * block_size.	"nrow, height, ydim"	nA := 4 * block_size.	"ncol, width, xdim"	mB := nA.	nB := 4 * block_size.	mC := mA.	nC := nB.	Transcript show: ('&lt;n&gt;mA, nA = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: mA with: nA).	Transcript show: ('&lt;n&gt;mB, nB = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: mB with: nB).	Transcript show: ('&lt;n&gt;mC, nC = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: mC with: nC).	h_A_ptr := CLimitedPrecisionRealType float gcCalloc: mA * nA.	0 to: (mA min: nA) - 1 do: [:i | h_A_ptr at: i * nA + i put: 1.0].	h_B_ptr := CLimitedPrecisionRealType float gcCalloc: mB * nB.	tempPtr := h_B_ptr copy.	1 to: mB * nB		do: 			[:i |			tempPtr contents: 0.1.			tempPtr += 1].	h_C_ptr := CLimitedPrecisionRealType float gcCalloc: mC * nC.	sizeOfFloat := CLimitedPrecisionRealType float sizeof.	cDeviceMemPtr := OrderedCollection new.	d_A_ptr := ICUDA CUdeviceptr gcCalloc.	cDeviceMemPtr add: d_A_ptr.	ICUDA		checkCudaErrors: [ICUDA cuMemAlloc: d_A_ptr with: sizeOfFloat * mA * nA].	d_B_ptr := ICUDA CUdeviceptr gcCalloc.	cDeviceMemPtr add: d_B_ptr.	ICUDA		checkCudaErrors: [ICUDA cuMemAlloc: d_B_ptr with: sizeOfFloat * mB * nB].	d_C_ptr := ICUDA CUdeviceptr gcCalloc.	cDeviceMemPtr add: d_C_ptr.	ICUDA		checkCudaErrors: [ICUDA cuMemAlloc: d_C_ptr with: sizeOfFloat * mC * nC].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_A_ptr contents				with: h_A_ptr				with: sizeOfFloat * mA * nA].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_B_ptr contents				with: h_B_ptr				with: sizeOfFloat * mB * nB].	nA_ptr := (CIntegerType int gcCalloc)				contents: nA;				yourself.	nB_ptr := (CIntegerType int gcCalloc)				contents: nB;				yourself.	kernelParams := CVoidType void pointerType gcCalloc: 5.	kernelParams		at: 0 put: d_C_ptr;		at: 1 put: d_A_ptr;		at: 2 put: d_B_ptr;		at: 3 put: nA_ptr;		at: 4 put: nB_ptr.	blockDimX := block_size.	blockDimY := block_size.	blockDimZ := 1.	gridDimX := (nC + blockDimX - 1) // blockDimX.	gridDimY := (mC + blockDimY - 1) // blockDimY.	gridDimZ := 1.	sharedMemBytes := 2 * block_size * block_size * sizeOfFloat.	self launchKernel.	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyDtoH: h_C_ptr				with: d_C_ptr contents				with: sizeOfFloat * mC * nC].	tempPtr := h_C_ptr copy.	answer := Array new: mC * nC.	1 to: mC * nC		do: 			[:i |			answer at: i put: tempPtr contents.			tempPtr += 1].	answer inspect.	cDeviceMemPtr		do: [:devicePtr | ICUDA checkCudaErrors: [ICUDA cuMemFree: devicePtr contents]]]			ensure: [self destroyCUDAContext]</body><body package="StCUDA">exampleMatrixMulNvrtcGC	"self new exampleMatrixMulNvrtcGC."	"C:\ProgramData\NVIDIA Corporation\CUDA Samples\v9.1\0_Simple\matrixMul_nvrtc"	"Aij*Bjk = Cik"	| h_A_ptr tempPtr h_B_ptr h_C_ptr d_A_ptr d_B_ptr d_C_ptr cDeviceMemPtr answer block_size mA nA mB nB mC nC sizeOfFloat nA_ptr nB_ptr |	Transcript show: ('&lt;n&gt;&lt;n&gt;&lt;1p&gt;'				expandMacrosWith: 'StCUDAMatrixMul new exampleMatrixMulNvrtcGC.').	ICUDA cuInit0.	self findBestDevID.	self createCUDAContext.		[ptxOrcuFile := '.\StCUDA\matrixMul_kernel (2).cu'.	compileOptions := Array with: '--include-path=./StCUDA/'.	self compileProgram.	self jitOptionsWithDefaults.	self createModule.	functionName := 'matrixMulCUDA_block32'.	"block_size must match block32"	self createFunction.	block_size := 32.	mA := 10 * block_size.	"nrow, height, ydim"	nA := 10 * block_size.	"ncol, width, xdim"	mB := 10 * block_size.	nB := 20 * block_size.	mC := mA.	nC := nB.	Transcript show: ('&lt;n&gt;mA, nA = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: mA with: nA).	Transcript show: ('&lt;n&gt;mB, nB = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: mB with: nB).	Transcript show: ('&lt;n&gt;mC, nC = &lt;1p&gt; &lt;2p&gt;' expandMacrosWith: mC with: nC).	h_A_ptr := CLimitedPrecisionRealType float gcCalloc: mA * nA.	0 to: (mA min: nA) - 1 do: [:i | h_A_ptr at: i * nA + i put: 1.0].	h_B_ptr := CLimitedPrecisionRealType float gcCalloc: mB * nB.	tempPtr := h_B_ptr copy.	1 to: mB * nB		do: 			[:i |			tempPtr contents: i.			tempPtr += 1].	h_C_ptr := CLimitedPrecisionRealType float gcCalloc: mC * nC.	sizeOfFloat := CLimitedPrecisionRealType float sizeof.	cDeviceMemPtr := OrderedCollection new.	d_A_ptr := ICUDA CUdeviceptr gcCalloc.	cDeviceMemPtr add: d_A_ptr.	ICUDA		checkCudaErrors: [ICUDA cuMemAlloc: d_A_ptr with: sizeOfFloat * mA * nA].	d_B_ptr := ICUDA CUdeviceptr gcCalloc.	cDeviceMemPtr add: d_B_ptr.	ICUDA		checkCudaErrors: [ICUDA cuMemAlloc: d_B_ptr with: sizeOfFloat * mB * nB].	d_C_ptr := ICUDA CUdeviceptr gcCalloc.	cDeviceMemPtr add: d_C_ptr.	ICUDA		checkCudaErrors: [ICUDA cuMemAlloc: d_C_ptr with: sizeOfFloat * mC * nC].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_A_ptr contents				with: h_A_ptr				with: sizeOfFloat * mA * nA].	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyHtoD: d_B_ptr contents				with: h_B_ptr				with: sizeOfFloat * mB * nB].	nA_ptr := (CIntegerType int gcCalloc)				contents: nA;				yourself.	nB_ptr := (CIntegerType int gcCalloc)				contents: nB;				yourself.	kernelParams := CVoidType void pointerType gcCalloc: 5.	kernelParams		at: 0 put: d_C_ptr;		at: 1 put: d_A_ptr;		at: 2 put: d_B_ptr;		at: 3 put: nA_ptr;		at: 4 put: nB_ptr.	blockDimX := block_size.	blockDimY := block_size.	blockDimZ := 1.	gridDimX := (nC + blockDimX - 1) // blockDimX.	gridDimY := (mC + blockDimY - 1) // blockDimY.	gridDimZ := 1.	sharedMemBytes := 0.	self launchKernel.	ICUDA checkCudaErrors: 			[ICUDA				cuMemcpyDtoH: h_C_ptr				with: d_C_ptr contents				with: sizeOfFloat * mC * nC].	tempPtr := h_C_ptr copy.	answer := Array new: mC * nC.	1 to: mC * nC		do: 			[:i |			answer at: i put: tempPtr contents.			tempPtr += 1].	answer inspect.	cDeviceMemPtr		do: [:devicePtr | ICUDA checkCudaErrors: [ICUDA cuMemFree: devicePtr contents]]]			ensure: [self destroyCUDAContext]</body></methods><methods><class-id>StCUDAWin64</class-id> <category>constants</category><body package="StCUDA">CUDA_SUCCESS	^0</body><body package="StCUDA">CU_COMPUTEMODE_PROHIBITED	^2</body><body package="StCUDA">CU_DEVICE_ATTRIBUTE_CLOCK_RATE	^13</body><body package="StCUDA">CU_DEVICE_ATTRIBUTE_COMPUTE_MODE	^20</body><body package="StCUDA">CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT	^16</body><body package="StCUDA">CU_JIT_INFO_LOG_BUFFER	^3</body><body package="StCUDA">CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES	^4</body><body package="StCUDA">CU_JIT_MAX_REGISTERS	^0</body><body package="StCUDA">NVRTC_SUCCESS	^0</body></methods><methods><class-id>StCUDAWin64</class-id> <category>helpers</category><body package="StCUDA">_ConvertSMVer2CoresDRV: major with: minor	| dict |	dict := IdentityDictionary new.	dict		at: (3 bitShift: 4) + 0 put: 192;		at: (3 bitShift: 4) + 2 put: 192;		at: (3 bitShift: 4) + 5 put: 192;		at: (3 bitShift: 4) + 7 put: 192;		at: (5 bitShift: 4) + 0 put: 128;		at: (5 bitShift: 4) + 2 put: 128;		at: (5 bitShift: 4) + 3 put: 128;		at: (6 bitShift: 4) + 0 put: 64;		at: (6 bitShift: 4) + 1 put: 128;		at: (6 bitShift: 4) + 2 put: 128;		at: (7 bitShift: 4) + 0 put: 64;		yourself.	^dict at: (major bitShift: 4) + minor</body><body package="StCUDA">checkCudaErrors: aBlock	"CUDA Driver API."	| returnCode |	returnCode := aBlock value.	returnCode ~= self CUDA_SUCCESS		ifTrue: 			[self cudaError_enum				membersDo: [:key :value | value = returnCode ifTrue: [self halt: key]]]</body><body package="StCUDA">checkNvrtcErrors: aBlock	"CUDA Runtime Compiler API."	| returnCode |	returnCode := aBlock value.	returnCode ~= self NVRTC_SUCCESS		ifTrue: 			[self nvrtcResult_enum				membersDo: [:key :value | value = returnCode ifTrue: [self halt: key]]]</body><body package="StCUDA">cuInit0	"self new cuInit0."	self checkCudaErrors: [self cuInit: 0]</body></methods><initialize><class-id>StCUDA</class-id></initialize><do-it>"Imported Classes:"</do-it><do-it>self error: 'Attempting to file-in parcel imports.  Choose terminate or close'</do-it><class><name>Object</name><environment>Core</environment><super></super><private>false</private><indexed-type>none</indexed-type><inst-vars></inst-vars><class-inst-vars></class-inst-vars><imports></imports><category>Kernel-Objects</category><attributes><package>Kernel-Objects</package></attributes></class><class><name>ExternalInterface</name><environment>External</environment><super>Core.Object</super><private>false</private><indexed-type>none</indexed-type><inst-vars></inst-vars><class-inst-vars>includeFiles includeDirectories libraryFiles libraryDirectories virtual optimizationLevel </class-inst-vars><imports>			private Kernel.OpcodePool.*			</imports><category>External-Interface</category><attributes><package>External-Interface</package></attributes></class></st-source>